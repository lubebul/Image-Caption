{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import _pickle as cPickle\n",
    "import os\n",
    "import multiprocessing as mp\n",
    "\n",
    "import scipy.misc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from pre_trained.cnn import PretrainedCNN\n",
    "import tensorflow as tf\n",
    "\n",
    "from IPython.display import display, SVG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess: Image\n",
    "\n",
    "Since the raw image takes about 20GB and may take days to download all of them. It's not included in the released file. But if you'd like to download origin image, you can request MS-COCO on-the-fly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def download_image(img_dir, img_id):\n",
    "    # download MS-COCO image\n",
    "    urllib.request.urlretrieve('http://mscoco.org/images/{}'.format(img_id.split('.')[0]), os.path.join(img_dir, img_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning: pre-trained CNN\n",
    "Our task, image captioning, requires good understanding of images, like\n",
    " * objects appeared in the image\n",
    " * relative positions of objects\n",
    " * colors, sizes, ...etc\n",
    "\n",
    "Training a good CNN from scratch is challenging and time-consuming, so we'll use existing pre-trained CNN model. The one we've prepared for you is the winner of 2012-ILSVRC model - VGG-16(or OxfordNet) in [pre_trained/cnn.py](pre_trained/cnn.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open('model_ckpt/cnn-model.svg', 'rb') as f:\n",
    "    arch = f.read()\n",
    "display(SVG(arch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VGG-16 consists of 16 layers, and we'll take the output of fc2 - the last layer before prediction layer, as input to our image-captioning model. However, since we have about 120,000 images, representing each image by 4,096 dimensions will make training inefficient and space-consuming. Therefore, dimensionality reduction techniques - PCA is used to reduce image feature dimension from 4096 to 256."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In summary, for each image, we do the following steps:\n",
    " 1. raw image is fed into VGG-16\n",
    " 2. take the output of second last layer\n",
    " 3. apply PCA to reduce dimension to 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess: Image (step 1)\n",
    "\n",
    "The pre-trained VGG-16 is taken from this [repository](https://github.com/machrisaa/tensorflow-vgg), which is converted to numpy format from origin Caffe model. However, there's a requirement for input image: the image must be\n",
    " * center cropped to $224\\times224$\n",
    " * substracted mean image\n",
    " * converted to BGR format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# step 1\n",
    "def process_image(img_dir, img_id):\n",
    "    MEAN = np.array([103.939, 116.779, 123.68]).astype(np.float32) # BGR\n",
    "    # crop image to 224 x 224 x 3 numpy array\n",
    "    path = os.path.join(img_dir, img_id)\n",
    "    if not os.path.exists(path):\n",
    "        download_image(img_dir, img_id)\n",
    "    img = scipy.misc.imread(path)\n",
    "    # center crop\n",
    "    short_edge = min(img.shape[:2])\n",
    "    yy = int((img.shape[0] - short_edge) / 2)\n",
    "    xx = int((img.shape[1] - short_edge) / 2)\n",
    "    crop_img = img[yy: yy + short_edge, xx: xx + short_edge]\n",
    "    img = scipy.misc.imresize(crop_img, (224,224,1))\n",
    "    img = img.reshape((224,224,1)) if len(img.shape) < 3 else img\n",
    "    if img.shape[2] < 3:\n",
    "        print('{}: dimension insufficient'.format(path))\n",
    "        img = img.reshape((224*224,img.shape[2])).T.reshape((img.shape[2], 224*224))\n",
    "        for i in range(img.shape[0], 3):\n",
    "            img = np.vstack([img, img[0,:]])\n",
    "        img = img.reshape((3,224*224)).T.reshape((224,224,3))\n",
    "    img = img.astype(np.float32)\n",
    "    img = img[:,:,::-1]\n",
    "    # RGB => BGR\n",
    "    for i in range(3):\n",
    "        img[:,:,i] -= MEAN[i]\n",
    "    return img.reshape((224,224,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess: Image (step 2)\n",
    "\n",
    "feed processed image into VGG-16 and save the extracted feature to [dataset/train_img4096.pkl](dataset/train_img4096.pkl) and [dataset/test_img4096.pkl](dataset/test_img4096.pkl)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# step 2\n",
    "def cnn_output(cnn_mdl, img_dir, img_ids, fout, bsize=100, ckpt_every=5000, frac=0.3, dev=0):\n",
    "    # extract output of pre-trained CNN model\n",
    "    fs, ims, outs, c = [], [], None, 0\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.per_process_gpu_memory_fraction=frac\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"]='{}'.format(dev)\n",
    "    with tf.Session(config=config) as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        for img_id in img_ids:\n",
    "            im = process_image(img_dir, img_id)\n",
    "            ims.append(im)\n",
    "            fs.append(img_id)\n",
    "            c += 1\n",
    "            if c % bsize == 0:\n",
    "                print('{} done'.format(c))\n",
    "                out = cnn_mdl.get_output(sess, ims)[0]\n",
    "                outs = out if outs is None else np.vstack([outs, out])\n",
    "                ims = []\n",
    "                if c % ckpt_every == 0:\n",
    "                    cPickle.dump({fs[i]:outs[i,:] for i in range(len(fs))}, open(fout, 'wb'))\n",
    "        if c % bsize != 0:\n",
    "            out = cnn_mdl.get_output(sess, ims)[0]\n",
    "            outs = out if outs is None else np.vstack([outs, out])\n",
    "        cPickle.dump({fs[i]:outs[i,:] for i in range(len(fs))}, open(fout, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To speedup the processing time, we use multiple GPUs at the same time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parallel(i, n_gpu=3):\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"]='{}'.format(int(i%n_gpu))\n",
    "    vgg = PretrainedCNN('pre_trained/vgg16_mat.pkl')\n",
    "    def run(train_test):\n",
    "        ids = pd.read_csv('dataset/{}.csv'.format(train_test))['img_id']\n",
    "        l = len(ids)\n",
    "        bsize = int((l+n_gpu-1)/n_gpu)\n",
    "        cnn_output(vgg, 'dataset/image', list(set(ids[bsize*i+th:bsize*(i+1)])), 'dataset/{}_img4096-{}.pkl'.format(train_test, i), bsize=100, frac=0.99, dev=(int(i%3)))\n",
    "    run('train')\n",
    "    run('test')\n",
    "    \n",
    "n_gpu = 3\n",
    "pool=mp.Pool(processes=n_gpu)\n",
    "pool.map(parallel, range(n_gpu))\n",
    "pool.close()\n",
    "pool.join()\n",
    "\n",
    "train_img = {k:v for k,v in cPickle.load(open('dataset/train_img4096-{}.pkl'.format(i), 'rb')) for i in range(n_gpu)}\n",
    "test_img = {k:v for k,v in cPickle.load(open('dataset/test_img4096-{}.pkl'.format(i), 'rb')) for i in range(n_gpu)}\n",
    "\n",
    "cPickle.dump(train_img, open('train_img4096.pkl', 'wb'))\n",
    "cPickle.dump(test_img, open('test_img4096.pkl', 'wb'))\n",
    "\n",
    "for i in range(n_gpu):\n",
    "    os.remove('dataset/train_img4096-{}.pkl'.format(i))\n",
    "    os.remove('dataset/test_img4096-{}.pkl'.format(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess: Image (step 3)\n",
    "\n",
    "Reduce dimension of image feature from 4096 to 256 and save reduced image feature as [dataset/train_img256.pkl](dataset/train_img256.pkl) and [dataset/test_img256.pkl](dataset/test_img256.pkl)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# step 3\n",
    "def pca(fin, fout, pca_transformer):\n",
    "    img = cPickle.load(open(fin, 'rb'))\n",
    "    K, V = [], []\n",
    "    for k,v in img.items():\n",
    "        K.append(k)\n",
    "        V.append(v)\n",
    "    x = np.array(V)\n",
    "    print(x.shape)\n",
    "    if pca_transformer is None:\n",
    "        pca_transformer = PCA(n_components=256)\n",
    "        pca_transformer.fit(x)\n",
    "    x = pca_transformer.transform(x)\n",
    "    for k,i in zip(K,range(len(K))):\n",
    "        img[k] = x[i]\n",
    "    cPickle.dump(img, open(fout, 'wb'))\n",
    "    return pca_transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# perform PCA\n",
    "pca_tf = None\n",
    "pca_tf = pca('dataset/train_img4096.pkl', 'dataset/train_img256.pkl', pca_tf)\n",
    "pca_tf = pca('dataset/test_img4096.pkl', 'dataset/test_img256.pkl', pca_tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The matrix $U$ used to perform PCA transforming is saved at [dataset/U.pkl](dataset/U.pkl)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get PCA transformer matrix\n",
    "img_train = cPickle.load(open('dataset/train_img256.pkl', 'rb'))\n",
    "img_4096 = cPickle.load(open('dataset/train_img4096.pkl', 'rb'))\n",
    "\n",
    "c = 0\n",
    "V4096, V256 = [], []\n",
    "for k,v in img_4096.items():\n",
    "    V4096.append(v)\n",
    "    V256.append(img_train[k])\n",
    "    c += 1\n",
    "    if c == 500:\n",
    "        break\n",
    "V4096 = np.array(V4096)\n",
    "V256 = np.array(V256)\n",
    "U = np.dot(np.linalg.pinv(V4096), V256)\n",
    "cPickle.dump(U, open('dataset/U.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It should be enough for you to train a good image-captioning model. However, you're always welcome to use other CNN models to extract image features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess: Text\n",
    "\n",
    "Dealing with raw strings is efficient, so we'll train on an encoded version of the captions. All necessary vocabularies is extracted in [dataset/text/vocab.pkl](dataset/text/vocab.pkl) and we'd like to represent captions by a sequence of integer IDs. However, since the length of captions may vary, our model needs to know where to start and stop. We'll append 2 special tokens `<ST>` and `<ED>` to the beginning and end of each caption. Also, the smaller the vocabulary size is the more efficient training will be, so we'll remove rare words by replacing rare words by `<RARE>` token. In summary, we'll going to \n",
    " * append `<ST>` and `<ED>` token to the beginning and end of each caption\n",
    " * replace rare words by `<RARE>` token\n",
    " * represent captions by vocabulary IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocab = cPickle.load(open('dataset/text/vocab.pkl', 'rb'))\n",
    "print('total {} vocabularies'.format(len(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def count_vocab_occurance(vocab, df):\n",
    "    voc_cnt = {v:0 for v in vocab}\n",
    "    for img_id, row in df.iterrows():\n",
    "        for w in row['caption'].split(' '):\n",
    "            voc_cnt[w] += 1\n",
    "    return voc_cnt\n",
    "\n",
    "df_train = pd.read_csv(os.path.join('dataset', 'train.csv'))\n",
    "\n",
    "print('count vocabulary occurances...')\n",
    "voc_cnt = count_vocab_occurance(vocab, df_train)\n",
    "\n",
    "# remove words appear < 100 times\n",
    "thrhd = 100\n",
    "x = np.array(list(voc_cnt.values()))\n",
    "print('{} words appear >= 100 times'.format(np.sum(x[(-x).argsort()] >= thrhd)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_voc_mapping(voc_cnt, thrhd):\n",
    "    \"\"\"\n",
    "    enc_map: voc --encode--> id\n",
    "    dec_map: id --decode--> voc\n",
    "    \"\"\"\n",
    "    def add(enc_map, dec_map, voc):\n",
    "        enc_map[voc] = len(dec_map)\n",
    "        dec_map[len(dec_map)] = voc\n",
    "        return enc_map, dec_map\n",
    "    # add <ST>, <ED>, <RARE>\n",
    "    enc_map, dec_map = {}, {}\n",
    "    for voc in ['<ST>', '<ED>', '<RARE>']:\n",
    "        enc_map, dec_map = add(enc_map, dec_map, voc)\n",
    "    for voc, cnt in voc_cnt.items():\n",
    "        if cnt < thrhd: # rare words => <RARE>\n",
    "            enc_map[voc] = enc_map['<RARE>']\n",
    "        else:\n",
    "            enc_map, dec_map = add(enc_map, dec_map, voc)\n",
    "    return enc_map, dec_map\n",
    "\n",
    "enc_map, dec_map = build_voc_mapping(voc_cnt, thrhd)\n",
    "# save enc/decoding map to disk\n",
    "cPickle.dump(enc_map, open('dataset/text/enc_map.pkl', 'wb'))\n",
    "cPickle.dump(dec_map, open('dataset/text/dec_map.pkl', 'wb'))\n",
    "vocab_size = len(dec_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def caption_to_ids(enc_map, df):\n",
    "    img_ids, caps = [], []\n",
    "    for idx, row in df.iterrows():\n",
    "        icap = [enc_map[x] for x in row['caption'].split(' ')]\n",
    "        icap.insert(0, enc_map['<ST>'])\n",
    "        icap.append(enc_map['<ED>'])\n",
    "        img_ids.append(row['img_id'])\n",
    "        caps.append(icap)\n",
    "    return pd.DataFrame({'img_id':img_ids, 'caption':caps}).set_index(['img_id'])\n",
    "\n",
    "\n",
    "enc_map = cPickle.load(open('dataset/text/enc_map.pkl', 'rb'))\n",
    "print('[transform captions into sequences of IDs]...')\n",
    "df_proc = caption_to_ids(enc_map, df_train)\n",
    "df_proc.to_csv('dataset/text/train_enc_cap.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def decode(dec_map, ids):\n",
    "    # decode IDs back to origin caption string\n",
    "    return ' '.join([dec_map[x] for x in ids])\n",
    "\n",
    "dec_map = cPickle.load(open('dataset/text/dec_map.pkl', 'rb'))\n",
    "\n",
    "print('And you can decode back easily to see full sentence...\\n')\n",
    "for idx, row in df_proc.iloc[:8].iterrows():\n",
    "    print('{}: {}'.format(idx, decode(dec_map, eval(row['caption']))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_embedding_matrix(w2v_path, dec_map, lang_dim=100):\n",
    "    out_vocab = []\n",
    "    embeddings_index = {}\n",
    "    f = open(w2v_path, 'r')\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "    # prepare embedding matrix\n",
    "    embedding_matrix = np.random.rand(len(dec_map), lang_dim)\n",
    "    for idx, wd in dec_map.items():\n",
    "        if wd in embeddings_index.keys():\n",
    "            embedding_matrix[idx] = embeddings_index[wd]\n",
    "        else:\n",
    "            out_vocab.append(wd)\n",
    "    print('words: \"{}\" not in pre-trained vocabulary list'.format(','.join(out_vocab)))\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning: pre-trained word embedding\n",
    "Image captioning also requires good unstanding of word meaning, so it's a good idea to use pre-trained word embedding. We'll take advantages of the released by Google - [GloVe](http://nlp.stanford.edu/projects/glove). As an example, we choose to use the smallest release [pre_trained/glove.6B.100d.txt](pre_trained/glove.6B.100d.txt), which is trained on 6 billion corpus of Wikipedia and Gigaword. Again, you're welcomed to use any pre-trained word embedding.<br>\n",
    "\n",
    "We'll pick the embedding vector of our vocabularies and save the embedding matrix in [dataset/text/embedding_matrix.pkl](dataset/text/embedding_matrix.pkl)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate_embedding_matrix(w2v_path, dec_map, lang_dim=100):\n",
    "    out_vocab = []\n",
    "    embeddings_index = {}\n",
    "    f = open(w2v_path, 'r')\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "    # prepare embedding matrix\n",
    "    embedding_matrix = np.random.rand(len(dec_map), lang_dim)\n",
    "    for idx, wd in dec_map.items():\n",
    "        if wd in embeddings_index.keys():\n",
    "            embedding_matrix[idx] = embeddings_index[wd]\n",
    "        else:\n",
    "            out_vocab.append(wd)\n",
    "    print('words: \"{}\" not in pre-trained vocabulary list'.format(','.join(out_vocab)))\n",
    "    return embedding_matrix\n",
    "\n",
    "dec_map = cPickle.load(open('dataset/text/dec_map.pkl', 'rb'))\n",
    "embedding_matrix = generate_embedding_matrix('pre_trained/glove.6B.100d.txt', dec_map)\n",
    "cPickle.dump(embedding_matrix, open('dataset/text/embedding_matrix.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
